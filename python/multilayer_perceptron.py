# -*- coding: utf-8 -*-
"""multilayer_perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vNZ0V7ZldKSNbXZAG51IxXvOo2-HOiC7
"""

import torch
from torch import nn, optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# define the model (fully connected)
class MLPerceptron(nn.Module):
    def __init__(self):
        super(MLPerceptron, self).__init__()
        self.layer1 = nn.Linear(784, 100)
        self.layer2 = nn.Linear(100, 100)
        self.layer3 = nn.Linear(100, 10)

    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = torch.relu(self.layer2(x))
        x = self.layer3(x)
        return x

# define a transform to normalize the data
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.5,), (0.5,))])

# download and load the training data
trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)
train_loader = DataLoader(trainset, batch_size=64, shuffle=True)

# instantiate the model, loss criterion, and optimizer
model = MLPerceptron()
loss_criterion = nn.MSELoss() # mean squared error loss function
optimizer = optim.SGD(model.parameters(), lr=0.01) # stochastic gradient descent

# a single training step
for inputs, labels in train_loader:
    # zero the parameter gradients
    optimizer.zero_grad()

    # reshape the inputs and forward pass
    inputs = inputs.view(inputs.shape[0], -1)
    outputs = model(inputs)
    
    # you might need to reshape or transform your labels to match the output shape
    labels = nn.functional.one_hot(labels, num_classes=10).float()
    
    # calculate loss
    loss = loss_criterion(outputs, labels)
    
    # backward pass and optimization
    loss.backward()
    optimizer.step()

print(model)
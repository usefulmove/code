{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "construct and train model -\n",
        "multi-layer perceptron neural network model implementation on MNIST data"
      ],
      "metadata": {
        "id": "tPoGEXrXKsId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# define the model\n",
        "class MLPerceptron(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLPerceptron, self).__init__()\n",
        "        input_size = 28 * 28\n",
        "        hidden_size = 128\n",
        "        output_size = 10\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer4 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = torch.relu(self.layer2(x))\n",
        "        x = torch.relu(self.layer3(x))\n",
        "        x = self.layer4(x)\n",
        "        return x\n",
        "\n",
        "# define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# download and load the data\n",
        "dataset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "\n",
        "# split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))  # 80% for training\n",
        "valid_size = len(dataset) - train_size  # 20% for validation\n",
        "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "# create training and validation data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# instantiate the model, loss criterion, and optimizer\n",
        "model = MLPerceptron()\n",
        "loss_criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.8)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# number of epochs\n",
        "n_epochs = 50\n",
        "\n",
        "# training and validation\n",
        "for epoch in range(n_epochs):\n",
        "    # training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # reshape the inputs and forward pass\n",
        "        inputs = inputs.view(inputs.shape[0], -1)\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # transform labels to match the output shape\n",
        "        labels = nn.functional.one_hot(labels, num_classes=10).float()\n",
        "\n",
        "        # calculate loss\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    print('epoch: {} \\ttraining loss: {:.6f}'.format(epoch+1, train_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDcHRFbK2cQ0",
        "outputId": "769a13bc-f468-4848-e721-3b3321b8e855"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLPerceptron(\n",
            "  (layer1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (layer3): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (layer4): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "epoch: 1 \ttraining loss: 0.027897\n",
            "epoch: 2 \ttraining loss: 0.011972\n",
            "epoch: 3 \ttraining loss: 0.009035\n",
            "epoch: 4 \ttraining loss: 0.007542\n",
            "epoch: 5 \ttraining loss: 0.006481\n",
            "epoch: 6 \ttraining loss: 0.005592\n",
            "epoch: 7 \ttraining loss: 0.005077\n",
            "epoch: 8 \ttraining loss: 0.004473\n",
            "epoch: 9 \ttraining loss: 0.004135\n",
            "epoch: 10 \ttraining loss: 0.003778\n",
            "epoch: 11 \ttraining loss: 0.003496\n",
            "epoch: 12 \ttraining loss: 0.003235\n",
            "epoch: 13 \ttraining loss: 0.002951\n",
            "epoch: 14 \ttraining loss: 0.002765\n",
            "epoch: 15 \ttraining loss: 0.002563\n",
            "epoch: 16 \ttraining loss: 0.002414\n",
            "epoch: 17 \ttraining loss: 0.002231\n",
            "epoch: 18 \ttraining loss: 0.002134\n",
            "epoch: 19 \ttraining loss: 0.001969\n",
            "epoch: 20 \ttraining loss: 0.001833\n",
            "epoch: 21 \ttraining loss: 0.001696\n",
            "epoch: 22 \ttraining loss: 0.001633\n",
            "epoch: 23 \ttraining loss: 0.001558\n",
            "epoch: 24 \ttraining loss: 0.001390\n",
            "epoch: 25 \ttraining loss: 0.001359\n",
            "epoch: 26 \ttraining loss: 0.001340\n",
            "epoch: 27 \ttraining loss: 0.001194\n",
            "epoch: 28 \ttraining loss: 0.001139\n",
            "epoch: 29 \ttraining loss: 0.001068\n",
            "epoch: 30 \ttraining loss: 0.001030\n",
            "epoch: 31 \ttraining loss: 0.000953\n",
            "epoch: 32 \ttraining loss: 0.000866\n",
            "epoch: 33 \ttraining loss: 0.000811\n",
            "epoch: 34 \ttraining loss: 0.000817\n",
            "epoch: 35 \ttraining loss: 0.000747\n",
            "epoch: 36 \ttraining loss: 0.000697\n",
            "epoch: 37 \ttraining loss: 0.000653\n",
            "epoch: 38 \ttraining loss: 0.000620\n",
            "epoch: 39 \ttraining loss: 0.000574\n",
            "epoch: 40 \ttraining loss: 0.000576\n",
            "epoch: 41 \ttraining loss: 0.000528\n",
            "epoch: 42 \ttraining loss: 0.000519\n",
            "epoch: 43 \ttraining loss: 0.000488\n",
            "epoch: 44 \ttraining loss: 0.000450\n",
            "epoch: 45 \ttraining loss: 0.000440\n",
            "epoch: 46 \ttraining loss: 0.000410\n",
            "epoch: 47 \ttraining loss: 0.000397\n",
            "epoch: 48 \ttraining loss: 0.000374\n",
            "epoch: 49 \ttraining loss: 0.000368\n",
            "epoch: 50 \ttraining loss: 0.000338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model validation"
      ],
      "metadata": {
        "id": "hDoTqpwdPgCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# validation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in valid_loader:\n",
        "        inputs = inputs.view(inputs.shape[0], -1)\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        # get the predicted class for each sample in the batch\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        \n",
        "        # count total number of labels and correct predictions\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# calculate the percentage of correct predictions\n",
        "accuracy = correct / total * 100\n",
        "\n",
        "print('model accuracy: {:.2f}%'.format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRSxNIaWKNwZ",
        "outputId": "56b131c7-fb3b-4c9c-ad75-dc1293d18b42"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model accuracy: 98.02%\n"
          ]
        }
      ]
    }
  ]
}